{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Download, pre-process, and analyse HSAF-H60B satellite rainfall observations\n",
    "This tutorial guides you step by step to:\n",
    "\n",
    "1. Download the H60B files from the HSAF server.\n",
    "2. Preprocess the data so it can be used in your analyses (e.g., regridding to latitude/longitude, extracting an area of interest).\n",
    "3. Analyze rainfall for specific points or an entire region, including visualization and cumulative calculations.\n",
    "\n",
    "The goal is to help you become familiar with HSAF H60B files.\n",
    "\n",
    "What is HSAF H60B?\n",
    "- H60B provides rainfall estimates derived from MSG SEVIRI satellite data.\n",
    "- The information is updated every 15 minutes for Africa and Europe and has a resolution of 3 x 3 km."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import math\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "\n",
    "# Libraries for reprojection (changing geographic coordinate systems)\n",
    "import rioxarray \n",
    "from pyproj import CRS\n",
    "from pyresample.geometry import AreaDefinition\n",
    "try:\n",
    "    from satpy.readers.core._geos_area import get_area_extent\n",
    "except ImportError:\n",
    "    from satpy.readers._geos_area import get_area_extent\n",
    "\n",
    "# Libraries for data and file management\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, date\n",
    "from ftplib import FTP   # To download files from an FTP server\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Suppress/ignore warnings for a cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"All libraries have been successfully imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Set up data folder structure\n",
    "Organize the data folder structure using relative paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders to save our raw and processed data\n",
    "data_folder = Path(\"./h60b_data\")\n",
    "raw_folder = data_folder / \"raw\"\n",
    "processed_folder = data_folder / \"processed\"\n",
    "\n",
    "# Create the directories if they donâ€™t already exist\n",
    "raw_folder.mkdir(parents=True, exist_ok=True)\n",
    "processed_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Data folders created:\")\n",
    "print(f\"Raw data: {raw_folder}\")\n",
    "print(f\"Processed data: {processed_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: FTP connection setup\n",
    "- Please register at https://hsaf.meteoam.it/User/Register to obtain login credentials for the server to download HSAF-H60B data\n",
    "- Define the FTP server name, the folder path on the FTP server, as well as your username and password\n",
    "- FTP (File Transfer Protocol) is a protocol that allows file transfers between a local computer and a remote server over the Internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FTP server details for HSAF\n",
    "ftp_server = \"ftphsaf.meteoam.it\"\n",
    "ftp_directory = \"./h60B/h60_cur_mon_data/\"\n",
    "\n",
    "# For this tutorial, use your personal HSAF credentials\n",
    "username = \"INSERT YOUR CREDENTIALS!!!\"   \t\t# Replace with your actual username\n",
    "password = \"INSERT YOUR CREDENTIALS!!!\"   \t# Replace with your actual password\n",
    "\n",
    "print(f\"FTP Server: {ftp_server}\")\n",
    "print(f\"Directory on the FTP server: {ftp_directory}\")\n",
    "print(\"Note: Don't forget to replace the username/password with your actual HSAF credentials!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Connect to the FTP server and list available files\n",
    "This step is useful to know for which dates there is data available before downloading them.\n",
    "We use the ftplib library to:\n",
    "1. Connect to the HSAF FTP server using your credentials\n",
    "2. Navigate to the directory containing the H60B files\n",
    "3. List all files available in that directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_h60b_timestamp(filename):\n",
    "    \"\"\"\n",
    "    Extract the timestamp from an H60B filename.\n",
    "    \n",
    "    H60B files available on the HSAF server contain date and time information in their filename in the form: h60B_YYYYMMDD_HHMM_xxx.nc\n",
    "\n",
    "    This function extracts this information and converts it into a Python datetime object, \n",
    "    allowing you to sort files, select time intervals, or perform any other temporal operations in the script.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Split the filename by \"_\"\n",
    "        parts = filename.split(\"_\")\n",
    "        date_str = parts[1]                  # YYYYMMDD\n",
    "        time_str = parts[2].split(\".\")[0]    # HHMM\n",
    "        \n",
    "        # Combine date and time and convert to a datetime object\n",
    "        timestamp = datetime.strptime(f\"{date_str}{time_str}\", \"%Y%m%d%H%M\")\n",
    "        return timestamp\n",
    "    except:\n",
    "        # If the filename is invalid, return None\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_h60b_files_full():\n",
    "    \"\"\"\n",
    "    List all H60B files available on the HSAF FTP server\n",
    "    and display the first and last available file.\n",
    "\n",
    "    H60B files on the HSAF server are only kept for the most recent months.\n",
    "    This function allows you to:\n",
    "      1. Connect to the HSAF FTP server using your credentials.\n",
    "      2. List all H60B files in the directory.\n",
    "      3. Extract the date and time from each filename.\n",
    "      4. Identify the first and last available file.\n",
    "\n",
    "    This operation is important to know the data availability period\n",
    "    before starting an analysis or selecting files for a specific time range.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Connecting to HSAF FTP server...\")\n",
    "        with FTP(ftp_server) as ftp:\n",
    "            # Connect using credentials\n",
    "            ftp.login(username, password)\n",
    "            print(\"Connection successful!\")\n",
    "            \n",
    "            # Navigate to the directory containing H60B files\n",
    "            ftp.cwd(ftp_directory)\n",
    "            print(f\"Current directory on server: {ftp_directory}\")\n",
    "            \n",
    "            # Retrieve the list of all files in the directory\n",
    "            files = []\n",
    "            ftp.dir(lambda line: files.append(line.split()[-1]))\n",
    "            \n",
    "            # Keep only H60B files (starting with 'h60')\n",
    "            h60b_files = [f for f in files if f.startswith(\"h60\")]\n",
    "            print(f\"{len(h60b_files)} H60B files found on the server\")\n",
    "            \n",
    "            # Extract timestamps and filter out invalid files\n",
    "            timestamps_files = [(parse_h60b_timestamp(f), f) \n",
    "                                for f in h60b_files \n",
    "                                if parse_h60b_timestamp(f) is not None]\n",
    "            \n",
    "            if not timestamps_files:\n",
    "                print(\"No files with valid timestamps found.\")\n",
    "                return []\n",
    "            \n",
    "            # Sort files by ascending timestamp\n",
    "            timestamps_files.sort()\n",
    "            \n",
    "            # Get the first and last file\n",
    "            first_file = timestamps_files[0]\n",
    "            last_file  = timestamps_files[-1]\n",
    "            \n",
    "            print(f\"\\nFirst available file: {first_file[1]} ({first_file[0]})\")\n",
    "            print(f\"Last available file: {last_file[1]} ({last_file[0]})\")\n",
    "            \n",
    "            # Return the full sorted list of filenames\n",
    "            return [f[1] for f in timestamps_files]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during FTP connection: {e}\")\n",
    "        print(\"Make sure you have registered at https://hsaf.meteoam.it/ and have valid credentials\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the functions above to retrieve available files\n",
    "# We check how many data files are available and the covered period\n",
    "all_files = list_h60b_files_full()\n",
    "\n",
    "# Extract timestamps for each file\n",
    "timestamps_files = [(parse_h60b_timestamp(f), f) for f in all_files if parse_h60b_timestamp(f)]\n",
    "timestamps_files.sort()  # Sort in ascending order by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_files_by_steps(start_time, n_steps=1, timestamps_files=timestamps_files):\n",
    "    \"\"\"\n",
    "    Selects a defined number of files starting from a given time.\n",
    "    \n",
    "    Parameters:\n",
    "    - start_time : datetime, the starting time for selection\n",
    "    - n_steps : int, number of consecutive files to select\n",
    "    - timestamps_files : list of tuples (timestamp, filename), already sorted by date\n",
    "    \"\"\"\n",
    "    selected_files = []\n",
    "    last_ts = start_time  # starting point for the search\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        next_file = None\n",
    "        # Find the next available file after last_ts\n",
    "        for ts, f in timestamps_files:\n",
    "            if ts >= last_ts:\n",
    "                next_file = (ts, f)\n",
    "                break\n",
    "        \n",
    "        if next_file:\n",
    "            selected_files.append(next_file[1])                 # Add the file to the selection\n",
    "            last_ts = next_file[0] + timedelta(minutes=15)      # Move the threshold just after this file\n",
    "        else:\n",
    "            print(f\"No file available after {last_ts}\")\n",
    "            break\n",
    "    \n",
    "    return selected_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Selecting a Period for HSAF Data Import\n",
    "1. We choose a starting moment (`start_datetime`) from which we want to begin retrieving the data.\n",
    "2. We define the number of consecutive files to retrieve (`n_steps`). Each file corresponds to a 15-minute time step.\n",
    "3. The `select_files_by_steps` function automatically selects the available files that match this period.\n",
    "\n",
    "Note: The starting date must be between the first and last file available on the server, which we identified using `list_h60b_files_full()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_datetime = datetime(2025, 10, 8, 15, 0)   # Define the starting time to retrieve HSAF files, Format: datetime(Year, Month, Day, Hour, Minute)\n",
    "n_steps = 10                                    # Number of files to select, each time step corresponds to 15 minutes\n",
    "\n",
    "# Select files\n",
    "selected_files = select_files_by_steps(start_datetime, n_steps)\n",
    "\n",
    "# Display selected files with their timestamps\n",
    "print(\"\\nSelected files:\")\n",
    "for f in selected_files:\n",
    "    ts = parse_h60b_timestamp(f)\n",
    "    print(f\" - {ts} : {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Preprocessing HSAF Data\n",
    "\n",
    "We will now import HSAF precipitation data from the server. Before we can use and analyze it properly, some preprocessing is required.  \n",
    "An important step in this preprocessing is, for example, converting a satellite grid into a latitude-longitude (lat-lon) grid, which makes it easier to manipulate and visualize the data.\n",
    "\n",
    "In this tutorial, we will first work on a single time step.\n",
    "This allows us to clearly understand each step of the preprocessing. Afterwards, we can apply the same process repeatedly in a loop to pre-process a large number timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Downloading a Single File\n",
    "This step shows how to download a specific H60B file from the FTP server:  \n",
    "1. Check if the file already exists in the local folder to avoid re-downloading.  \n",
    "2. Download the compressed file (.gz) from the FTP server.  \n",
    "3. Decompress the file into NetCDF format (.nc).  \n",
    "4. Delete the compressed file to save disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_h60b_file(filename, output_folder=None):\n",
    "    \"\"\"Download and decompress a single H60B file\"\"\"\n",
    "\n",
    "    target_folder = Path(output_folder) if output_folder else raw_folder\n",
    "    target_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    gz_path = target_folder / filename\n",
    "    nc_path = target_folder / filename.replace(\".gz\", \"\")\n",
    "\n",
    "    # Check if the file already exists locally\n",
    "    if nc_path.exists():\n",
    "        print(f\"File already exists: {nc_path.name}\")\n",
    "        return str(nc_path)\n",
    "\n",
    "    try:\n",
    "        print(f\"Downloading {filename}...\")\n",
    "\n",
    "        # Download the compressed file from the FTP server\n",
    "        with FTP(ftp_server) as ftp:\n",
    "            ftp.login(username, password)\n",
    "            ftp.cwd(ftp_directory)\n",
    "\n",
    "            with open(gz_path, \"wb\") as f:\n",
    "                ftp.retrbinary(f\"RETR {filename}\", f.write)\n",
    "\n",
    "        file_size = gz_path.stat().st_size / (1024*1024)  # Size in MB\n",
    "        print(f\"Downloaded {filename} ({file_size:.1f} MB)\")\n",
    "\n",
    "        # Decompress the file\n",
    "        print(f\"Decompressing the file...\")\n",
    "        with gzip.open(gz_path, \"rb\") as f_in:\n",
    "            with open(nc_path, \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "        # Delete the compressed file to save disk space\n",
    "        gz_path.unlink()\n",
    "\n",
    "        final_size = nc_path.stat().st_size / (1024*1024)  # Final size in MB\n",
    "        print(f\"Decompressed to {nc_path.name} ({final_size:.1f} MB)\")\n",
    "\n",
    "        return str(nc_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {filename}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Download a single file by calling the function\n",
    "\n",
    "if selected_files:\n",
    "    latest_file = selected_files[-1]                    # Take the most recent file from the availability list\n",
    "    downloaded_file = download_h60b_file(latest_file)\n",
    "else:\n",
    "    print(\"No files available from the FTP\")\n",
    "    downloaded_file = None\n",
    "\n",
    "print(downloaded_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the folder `h60b_data/raw/`, you can now check if an HSAF file has been downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Inspecting the raw H60B Data\n",
    "1. Check the structure of the H60B file.\n",
    "2. See which variables and dimensions are available.\n",
    "3. Examine the precipitation variable 'rr' and its actual values.\n",
    "4. Review projection information, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_raw_data(filepath):\n",
    "    \"\"\"Inspect the structure of a raw H60B file\"\"\"\n",
    "    \n",
    "    if not filepath or not Path(filepath).exists():\n",
    "        print(\"No file available for inspection\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Inspecting raw H60B file: {Path(filepath).name}\")\n",
    "    \n",
    "    # Open the NetCDF dataset with xarray\n",
    "    ds = xr.open_dataset(filepath, decode_cf=False)\n",
    "    \n",
    "    print(f\"\\nDataset structure:\")\n",
    "    print(f\"   Dimensions : {dict(ds.dims)}\")        # e.g., time, x, y\n",
    "    print(f\"   Variables : {list(ds.data_vars)}\")    # e.g., rr (precipitation)\n",
    "    print(f\"   Coordinates : {list(ds.coords)}\")     # e.g., latitude, longitude\n",
    "    \n",
    "    # Examine the precipitation variable\n",
    "    rr = ds['rr']\n",
    "    print(f\"\\nPrecipitation variable 'rr':\")\n",
    "    print(f\"   Shape : {rr.shape}\")\n",
    "    print(f\"   Data type : {rr.dtype}\")\n",
    "    print(f\"   Raw values : from {rr.min().values} to {rr.max().values}\")\n",
    "    \n",
    "    # Check data parameters (scale factor, offset)\n",
    "    scale_factor = rr.encoding.get('scale_factor', 0.1)\n",
    "    add_offset = rr.encoding.get('add_offset', 0.0)\n",
    "    print(f\"   Scale factor : {scale_factor}\")\n",
    "    print(f\"   Added offset : {add_offset}\")\n",
    "    \n",
    "    # Compute the precipitation range after applying scale and offset\n",
    "    rr_scaled = rr.astype('float32') * scale_factor + add_offset  # rr stands for rainfall rate\n",
    "    print(f\"   Actual precipitation range : from {rr_scaled.min().values:.3f} to {rr_scaled.max().values:.3f} mm/h\")\n",
    "    \n",
    "    # Display projection information if available\n",
    "    if 'gdal_projection' in ds.attrs:\n",
    "        print(f\"\\nProjection : {ds.attrs['gdal_projection']}\")\n",
    "    \n",
    "    return ds\n",
    "\n",
    "raw_dataset = examine_raw_data(downloaded_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Visualizing Raw Data\n",
    "To validate the raw data, we will proceed as follows:\n",
    "\n",
    "1. Plot the precipitation on a map to observe its spatial distribution.\n",
    "2. Create a histogram to visualize the distribution of rainfall rates.\n",
    "3. Calculate some basic statistics to understand the intensity and extent of precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Visualizing raw H60B data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_raw_data(ds, filename=None):\n",
    "    \"\"\"Create a simple visualization of raw H60B data\"\"\"\n",
    "    \n",
    "    if ds is None:\n",
    "        print(\"No dataset to display\")\n",
    "        return\n",
    "    \n",
    "    # Extract date/time from the filename if provided\n",
    "    if filename is not None:\n",
    "        match = re.search(r'(\\d{8})_(\\d{4})', filename)\n",
    "        if match:\n",
    "            date_str, time_str = match.groups()\n",
    "            timestamp = f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]} {time_str[:2]}:{time_str[2:]}\"\n",
    "        else:\n",
    "            timestamp = \"Unknown\"\n",
    "    else:\n",
    "        timestamp = \"Unknown\"\n",
    "\n",
    "    print(f\"Displaying data for file: {filename} (time: {timestamp})\")\n",
    "\n",
    "    # Apply scale factor: used for storage reduction, does not affect results\n",
    "    rr = ds['rr']\n",
    "    scale_factor = rr.encoding.get('scale_factor', 0.1)\n",
    "    add_offset = rr.encoding.get('add_offset', 0.0)\n",
    "    precip = rr.astype('float32') * scale_factor + add_offset\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Precipitation map\n",
    "    im1 = ax1.imshow(precip.values, cmap='Blues', vmin=0, vmax=10)\n",
    "    ax1.set_title(f'H60B Precipitation Rate (mm/h)\\nTime: {timestamp}')\n",
    "    ax1.set_xlabel('Pixels X')\n",
    "    ax1.set_ylabel('Pixels Y')\n",
    "    plt.colorbar(im1, ax=ax1, label='mm/h')\n",
    "    \n",
    "    # Histogram\n",
    "    valid_precip = precip.values[precip.values >= 0]\n",
    "    ax2.hist(valid_precip[valid_precip > 0], bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax2.set_xlabel('Precipitation rate (mm/h)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Distribution of precipitation rate')\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\nPrecipitation statistics:\")\n",
    "    print(f\"   Total pixels : {precip.size:,}\")\n",
    "    print(f\"   Pixels with rain (>0 mm/h) : {np.sum(valid_precip > 0):,}\")\n",
    "    print(f\"   Rain coverage : {100 * np.sum(valid_precip > 0) / len(valid_precip):.1f}%\")\n",
    "    print(f\"   Maximum precipitation rate : {np.max(valid_precip):.1f} mm/h\")\n",
    "    print(f\"   Mean precipitation rate (rainy areas) : {np.mean(valid_precip[valid_precip > 0]):.2f} mm/h\")\n",
    "\n",
    "plot_raw_data(raw_dataset, filename=Path(downloaded_file).name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the satellite grid is in pixels X and Y, not in latitude/longitude.  \n",
    "Each cell in the H60B file corresponds to a pixel from the satellite sensor.  \n",
    "If you want to use this data on a geographic map or with QGIS, it is practical to reprojected the data onto a latitude/longitude grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Preprocessing Data and reproject onto a Latitude/Longitude Grid\n",
    "\n",
    "Main steps:\n",
    "1. Load the raw H60B file.\n",
    "2. Apply scale factors and rename the precipitation variable.\n",
    "3. Correctly define the CRS (coordinate reference system).\n",
    "4. Create consistent spatial coordinates.\n",
    "5. Reproject the data to WGS84 (latitude/longitude).\n",
    "6. Set global and variable metadata.\n",
    "7. Save the NetCDF file in the processed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_single_file(raw_file: str, output_folder=None):\n",
    "    \"\"\"Preprocess a single H60B file for WGS84 with consistent variable names.\"\"\"\n",
    "    \n",
    "    # Determine target folder for results: the default is processed_folder\n",
    "    target_folder = Path(output_folder) if output_folder else processed_folder\n",
    "    target_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    filename = Path(raw_file).name\n",
    "\n",
    "    # Extract timestamp from filename and define output path\n",
    "    timestamp = parse_h60b_timestamp(filename)\n",
    "    hsaf_filename = f\"HSAF-H60B_{timestamp.strftime('%Y%m%dT%H%M%S')}.nc\"\n",
    "    output_path = target_folder / hsaf_filename\n",
    "\n",
    "    # Check if preprocessed file already exists, if it already exists it is not needed to pre-process it again\n",
    "    if output_path.exists():\n",
    "        print(f\"File already preprocessed: {hsaf_filename} -> Skipping preprocessing\")\n",
    "        return str(output_path)\n",
    "\n",
    "    print(f\"Preprocessing H60B file: {filename} -> {hsaf_filename}\")\n",
    "    \n",
    "    # Define coordinate systems\n",
    "    crs_in = \"+proj=geos +a=6378.169 +b=6356.584 +h=35785.831 +lat_0=0 +lon_0=0.000000\"  # Original CRS (satellite)\n",
    "    crs_out = \"EPSG:4326\"  # Output CRS: latitude/longitude\n",
    "\n",
    "    # Load raw file without automatic CF decoding\n",
    "    ds = xr.open_dataset(raw_file, decode_coords=\"all\", decode_cf=False)\n",
    "\n",
    "    # Rename coordinates and apply scale factor\n",
    "    ds = ds.rename({\"nx\": \"x\", \"ny\": \"y\"})\n",
    "    rr_raw = ds[\"rr\"]\n",
    "    scale_factor = rr_raw.encoding.get(\"scale_factor\", 0.1)\n",
    "    add_offset  = rr_raw.encoding.get(\"add_offset\", 0.0)\n",
    "    ds[\"rr\"] = rr_raw.astype(\"float32\") * scale_factor + add_offset\n",
    "    ds[\"rr\"].encoding.clear()\n",
    "    ds[\"rr\"].attrs.clear()\n",
    "    ds = ds[[\"rr\"]].astype(\"float32\")\n",
    "\n",
    "    # Use CRS directly from file to preserve exact satellite parameters\n",
    "    source_crs = CRS.from_proj4(ds.attrs[\"gdal_projection\"])\n",
    "    crs_in = source_crs.to_proj4()\n",
    "    crs_out = \"EPSG:4326\"\n",
    "\n",
    "    # --- Define satellite area extent and coordinates\n",
    "    cgms_projection = (\n",
    "        \"+proj=geos +coff=1856.000000 +cfac=13642337.000000 \"\n",
    "        \"+loff=1856.000000 +lfac=13642337.000000 \"\n",
    "        \"+spp=0.000000 +r_eq=6378.169000 +r_pol=6356.583800 +h=42164.000000\"\n",
    "    )\n",
    "    matches = re.findall(r\"\\+?(\\w+)\\s*=\\s*([^\\s]+)\", cgms_projection)\n",
    "    parse_dict = {k: v for k, v in matches}\n",
    "\n",
    "    area_extent = get_area_extent({\n",
    "        \"scandir\": \"N2S\",\n",
    "        \"h\": float(parse_dict[\"h\"]) * 1000 - float(parse_dict[\"r_eq\"]) * 1000,\n",
    "        \"loff\": float(parse_dict[\"loff\"]),\n",
    "        \"coff\": float(parse_dict[\"coff\"]),\n",
    "        \"lfac\": float(parse_dict[\"lfac\"]),\n",
    "        \"cfac\": float(parse_dict[\"cfac\"]),\n",
    "        \"ncols\": ds.x.size,\n",
    "        \"nlines\": ds.y.size,\n",
    "    })\n",
    "\n",
    "    area_def_src = AreaDefinition(\n",
    "        \"areaD\",\n",
    "        source_crs.to_dict()[\"proj\"],\n",
    "        \"areaD\",\n",
    "        {\"lon_0\": source_crs.to_dict()[\"lon_0\"], \"a\": source_crs.to_dict()[\"a\"], \"b\": source_crs.to_dict()[\"b\"], \"h\": source_crs.to_dict()[\"h\"], \"proj\": source_crs.to_dict()[\"proj\"]},\n",
    "        ds.y.size,\n",
    "        ds.x.size,\n",
    "        (area_extent[0], area_extent[1], area_extent[2], area_extent[3]),\n",
    "    )\n",
    "\n",
    "    # Create new X and Y coordinates\n",
    "    x, y = area_def_src.get_proj_coords()\n",
    "    new_x_coords = np.linspace(x.min(), x.max(), num=ds.sizes[\"x\"])\n",
    "    new_y_coords = np.linspace(y.max(), y.min(), num=ds.sizes[\"y\"])\n",
    "    ds = ds.assign_coords(y=(\"y\", new_y_coords), x=(\"x\", new_x_coords))\n",
    "\n",
    "    # Write CRS matching the coordinate units\n",
    "    ds = ds.rio.write_crs(crs_in)\n",
    "\n",
    "    # Rename variable, add time, sort dimensions\n",
    "    ds = ds.rename({\"rr\": \"precip_intensity\"}).sortby(\"y\")\n",
    "    ds = ds.expand_dims(\"time\").assign_coords(time=(\"time\", [timestamp]))\n",
    "    ds = ds.transpose(\"time\", \"y\", \"x\")\n",
    "\n",
    "    # Set precipitation variable attributes\n",
    "    ds.precip_intensity.attrs = {\n",
    "        \"standard_name\": \"precipitation_flux\",\n",
    "        \"long_name\": \"Precipitation flux derived from cloud optical properties\",\n",
    "        \"units\": \"kg m-2 h-1\",\n",
    "    }\n",
    "\n",
    "    # Replace invalid values with NaN\n",
    "    ds[\"precip_intensity\"] = ds[\"precip_intensity\"].where(ds[\"precip_intensity\"] >= 0, np.nan)\n",
    "    da = ds[\"precip_intensity\"].rio.write_nodata(np.nan, encoded=True)\n",
    "    ds[\"precip_intensity\"] = da\n",
    "\n",
    "    print(\"Before reprojection: min/max\", float(ds.precip_intensity.min()), float(ds.precip_intensity.max()))\n",
    "\n",
    "    # Reproject to WGS84\n",
    "    print(\"Reprojecting H60B data to WGS84\")\n",
    "    ds = ds.rio.reproject(crs_out, nodata=np.nan)\n",
    "\n",
    "    print(\"After reprojection: min/max\", float(ds.precip_intensity.min(skipna=True)), float(ds.precip_intensity.max(skipna=True)))\n",
    "\n",
    "    # Clear encoding after reprojection\n",
    "    for var_name in ds.data_vars:\n",
    "        ds[var_name].encoding.clear()\n",
    "        for attr in [\"_FillValue\", \"missing_value\", \"fill_value\", \"FillValue\"]:\n",
    "            ds[var_name].attrs.pop(attr, None)\n",
    "\n",
    "    # Final dimension order\n",
    "    ds = ds.transpose(\"time\", \"y\", \"x\")\n",
    "\n",
    "    # Set CF attributes for coordinates\n",
    "    ds.x.attrs = {\"standard_name\": \"longitude\", \"long_name\": \"longitude\", \"units\": \"degrees_east\", \"axis\": \"X\"}\n",
    "    ds.y.attrs = {\"standard_name\": \"latitude\", \"long_name\": \"latitude\", \"units\": \"degrees_north\", \"axis\": \"Y\"}\n",
    "    ds.time.attrs = {\"standard_name\": \"time\", \"long_name\": \"time\", \"axis\": \"T\"}\n",
    "\n",
    "    # Set global dataset attributes\n",
    "    ds.attrs = {\n",
    "        \"Conventions\": \"CF-1.6\",\n",
    "        \"title\": \"RAINSAT H60B MSG SEVIRI Precipitation\",\n",
    "        \"source\": \"EUMETSAT H-SAF H60B\",\n",
    "        \"creator\": \"HKV services\",\n",
    "        \"creation_date\": date.today().strftime(\"%Y-%m-%d\"),\n",
    "        \"time_coverage_start\": timestamp.strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "        \"time_coverage_end\": (timestamp + timedelta(minutes=15)).strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "        \"geospatial_lat_min\": float(ds.y.min()),\n",
    "        \"geospatial_lat_max\": float(ds.y.max()),\n",
    "        \"geospatial_lon_min\": float(ds.x.min()),\n",
    "        \"geospatial_lon_max\": float(ds.x.max()),\n",
    "        \"crs\": crs_out,\n",
    "        \"product_details\": \"https://hsaf.meteoam.it/Products/Detail?prod=H60B\",\n",
    "        \"data_source\": \"hsaf-h60b\",\n",
    "    }\n",
    "\n",
    "    # Encoding for NetCDF\n",
    "    encoding = {\n",
    "        \"precip_intensity\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 4, \"_FillValue\": -999.0},\n",
    "        \"time\": {\"units\": \"minutes since 1970-01-01 00:00:00\", \"dtype\": \"float64\"},\n",
    "    }\n",
    "\n",
    "    # Save preprocessed NetCDF file\n",
    "    ds.to_netcdf(output_path, encoding=encoding)\n",
    "    ds.close()\n",
    "    print(f\"H60B file successfully preprocessed: {hsaf_filename}\")\n",
    "    return str(output_path)\n",
    "\n",
    "# Run preprocessing for the downloaded file\n",
    "processed_file = preprocess_single_file(downloaded_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Visualizing Preprocessed Data in Latitude and Longitude\n",
    "Now, we will create two plots to analyse the preprocessed data results:  \n",
    "1. A full precipitation map (mm/h).  \n",
    "2. A precipitation map which highlights only the rainy areas (> 0.1 mm/h).\n",
    "\n",
    "At the same time, we will display some statistics to better anlayse the data quality and coverage:  \n",
    "- The size of the used grid  \n",
    "- The number of valid pixels  \n",
    "- The number and coverage of rainy pixels  \n",
    "- Maximum and mean precipitation intensity in rainy areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_processed_data(processed_file):\n",
    "    \"\"\"Visualize preprocessed precipitation data\"\"\"\n",
    "    \n",
    "    # Load the preprocessed NetCDF file\n",
    "    ds = xr.open_dataset(processed_file)\n",
    "    print(ds)\n",
    "    \n",
    "    # Select the first time step (usually only one for H60B)\n",
    "    precip = ds[\"precip_intensity\"].isel(time=0)\n",
    "    \n",
    "    # Create figure with two side-by-side subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # --- Plot 1: Full precipitation map\n",
    "    im1 = precip.plot(ax=ax1, cmap='Blues', vmin=0, vmax=10, \n",
    "                      cbar_kwargs={'label': 'Precipitation (mm/h)'})\n",
    "    ax1.set_title('Preprocessed H60B Data')\n",
    "    ax1.set_aspect('equal')  # Ensure X and Y axes have same scale\n",
    "    \n",
    "    # --- Plot 2: Highlight only rainy areas\n",
    "    rain_mask = precip.values > 0.1  # Areas with > 0.1 mm/h\n",
    "    if np.any(rain_mask):\n",
    "        im2 = ax2.imshow(np.where(rain_mask, precip.values, np.nan), \n",
    "                         cmap='viridis', vmin=0.1)\n",
    "        ax2.set_title('Rainy Areas Only (> 0.1 mm/h)')\n",
    "        plt.colorbar(im2, ax=ax2, label='mm/h')\n",
    "    else:\n",
    "        # Display a message if no significant rain\n",
    "        ax2.text(0.5, 0.5, 'No significant\\nrain detected', \n",
    "                 ha='center', va='center', transform=ax2.transAxes)\n",
    "        ax2.set_title('Rainy Areas Only')\n",
    "    \n",
    "    ax2.set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # --- Summary statistics\n",
    "    valid_data = precip.values[precip.values >= 0]  # Valid pixels\n",
    "    rainy_data = valid_data[valid_data > 0.1]       # Pixels with rain > 0.1 mm/h\n",
    "    \n",
    "    print(f\"\\nSummary of preprocessed data:\")\n",
    "    print(f\"   Time : {ds.time.values[0]}\")\n",
    "    print(f\"   Grid size : {precip.shape[0]} x {precip.shape[1]} pixels\")\n",
    "    print(f\"   Valid pixels : {len(valid_data):,}\")\n",
    "    print(f\"   Rainy pixels (>0.1 mm/h) : {len(rainy_data):,}\")\n",
    "    print(f\"   Rain coverage : {100 * len(rainy_data) / len(valid_data):.2f}%\")\n",
    "    \n",
    "    if len(rainy_data) > 0:\n",
    "        print(f\"   Maximum intensity : {np.max(rainy_data):.1f} mm/h\")\n",
    "        print(f\"   Mean intensity (rainy areas) : {np.mean(rainy_data):.2f} mm/h\")\n",
    "\n",
    "# Run visualization if a preprocessed file is available\n",
    "if processed_file is not None:\n",
    "    plot_processed_data(processed_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You can observe the effect of the re-projecting process. In the left image the data shows a more round data coverage compaired to the raw data visualissation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11: Final Operational Loop\n",
    "We have now observed in detail how the pre-processing is organized for one single file/timestep. \n",
    "The next steps combines all previous steps into a single loop for efficient pre-processing over a number of timesteps:  \n",
    "1. Download the file from the FTP server  \n",
    "2. Preprocess the file to convert it into a latitude/longitude grid  \n",
    "3. Save the processed file to the appropriate folder  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier in this notebook, you selected a time period of interest for analyzing HSAF data.\n",
    "# These files will now be used in the following loop. The selected time steps are:\n",
    "selected_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final loop to download and preprocess the selected files\n",
    "for selected_file in selected_files:\n",
    "    # Determine the path of the downloaded file\n",
    "    downloaded_file = download_h60b_file(selected_file)\n",
    "    \n",
    "    # Extract timestamp from the filename to determine the output file\n",
    "    timestamp = parse_h60b_timestamp(Path(downloaded_file).name)\n",
    "    hsaf_filename = f\"HSAF-H60B_{timestamp.strftime('%Y%m%dT%H%M%S')}.nc\"\n",
    "    output_path = processed_folder / hsaf_filename\n",
    "\n",
    "    # Check if the file has already been preprocessed\n",
    "    if output_path.exists():\n",
    "        print(f\"{hsaf_filename} already exists, skipping to next file\\n\" + \"-\"*40)\n",
    "        continue\n",
    "\n",
    "    # Preprocess the file\n",
    "    preprocess_single_file(downloaded_file)\n",
    "    print(f\"{selected_file} processed\\n\" + \"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 12: Selecting and Analyzing HSAF Data for a Specific Area\n",
    "The HSAF files have been downloaded for a very large area. Now, we will zoom in and analyze the data for a specific area of interest,  \n",
    "using latitude and longitude coordinates to define your region of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available time steps in the folder containing preprocessed HSAF precipitation data.\n",
    "# You can choose which of these files you want to visualize.\n",
    "\n",
    "file_list = sorted(processed_folder.glob(\"HSAF-H60B_*.nc\"))\n",
    "for f in file_list:\n",
    "    print(f.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Plot a subset of H60B data for a specific region\n",
    "def plot_h60b_subset(processed_folder, filename=None, start_time=None, end_time=None, region=None, max_cols=5):\n",
    "    \"\"\"\n",
    "    Visualize preprocessed H60B data for a given region.\n",
    "    - filename : display a specific file\n",
    "    - start_time / end_time : display all time steps in this interval\n",
    "    - region : dictionary with lat_min, lat_max, lon_min, lon_max\n",
    "    - max_cols : maximum number of columns per row for subplots\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    if filename is not None:\n",
    "        ds = xr.open_dataset(processed_folder / filename)\n",
    "    else:\n",
    "        ds = xr.open_mfdataset(str(processed_folder / \"HSAF-H60B_*.nc\"))\n",
    "    \n",
    "    # Define the region\n",
    "    if region is None:\n",
    "        region = {\"lat_min\": -12, \"lat_max\": 22, \"lon_min\": 21, \"lon_max\": 55}  # Adjust to your area of interest\n",
    "    \n",
    "    ds_roi = ds.sel(\n",
    "        x=slice(region[\"lon_min\"], region[\"lon_max\"]),\n",
    "        y=slice(region[\"lat_max\"], region[\"lat_min\"])  # inverted for y\n",
    "    )\n",
    "    \n",
    "    # Filter by time interval\n",
    "    if start_time is not None and end_time is not None:\n",
    "        ds_roi = ds_roi.sel(time=slice(start_time, end_time))\n",
    "    \n",
    "    # Set bounds and colors for the colormap\n",
    "    bounds = [0, 0.5, 2, 5, 10, 15, 25, 40, 100]  # Precipitation intensity levels\n",
    "    colors = [\"#ffffff\", \"#add8e6\", \"#0000ff\", \"#00ff00\",\n",
    "              \"#ffff00\", \"#ffa500\", \"#ff0000\", \"#ff69b4\"]\n",
    "    cmap = mcolors.ListedColormap(colors)\n",
    "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "    \n",
    "    # Number of time steps\n",
    "    n_times = len(ds_roi.time)\n",
    "    n_cols = min(n_times, max_cols)\n",
    "    n_rows = math.ceil(n_times / max_cols)\n",
    "    \n",
    "    # Create figure with grid\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 5*n_rows), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    axes = np.array(axes).ravel() \n",
    "\n",
    "    # Loop through each time step\n",
    "    for t in range(n_times):\n",
    "        ax = axes[t]\n",
    "        subset = ds_roi.precip_intensity.isel(time=t)\n",
    "        \n",
    "        ax.add_feature(cfeature.BORDERS, linewidth=1)\n",
    "        ax.add_feature(cfeature.COASTLINE, linewidth=1)\n",
    "        ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "        \n",
    "        im = ax.pcolormesh(subset.x, subset.y, subset.values,\n",
    "                           cmap=cmap, norm=norm, shading='auto')\n",
    "        cbar = plt.colorbar(im, ax=ax, orientation='vertical', label='Precipitation (mm/h)')\n",
    "        cbar.set_ticks(bounds)\n",
    "        cbar.set_ticklabels([str(b) for b in bounds])\n",
    "        \n",
    "        ax.set_extent([region[\"lon_min\"], region[\"lon_max\"],\n",
    "                       region[\"lat_min\"], region[\"lat_max\"]])\n",
    "        \n",
    "        time_str = np.datetime_as_string(subset.time.values, unit='m')\n",
    "        ax.set_title(time_str)\n",
    "    \n",
    "    # Hide empty axes if fewer subplots than n_rows*n_cols\n",
    "    for t in range(n_times, n_rows*n_cols):\n",
    "        axes[t].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we call the visualization function for a specific time step.\n",
    "# Select a time step from the list of files in the processed data folder.\n",
    "plot_h60b_subset(processed_folder) #, filename=\"HSAF-H60B_20250810T080000.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 13: Extract HSAF precipitation for specific points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the folder containing the preprocessed HSAF files\n",
    "processed_folder = Path(\"h60b_data/processed/\")\n",
    "\n",
    "# 2. Define points of interest (latitude and longitude)\n",
    "points = [\n",
    "    {\"name\": \"Nairobi\", \"lat\": -1.286389, \"lon\": 36.817223},\n",
    "    {\"name\": \"Mombasa\", \"lat\": -4.043477, \"lon\": 39.668206},\n",
    "    {\"name\": \"Kisumu\", \"lat\": -0.091702, \"lon\": 34.767956},\n",
    "    {\"name\": \"Nakuru\", \"lat\": -0.303099, \"lon\": 36.080025},\n",
    "    {\"name\": \"Eldoret\", \"lat\": 0.514277, \"lon\": 35.269779},\n",
    "    # Add more points if needed\n",
    "]\n",
    "\n",
    "# 3. Load all preprocessed HSAF files\n",
    "ds = xr.open_mfdataset(str(processed_folder / \"HSAF-H60B_*.nc\"))\n",
    "\n",
    "# 4. Extract time series for each point\n",
    "data_dict = {}\n",
    "\n",
    "for pt in points:\n",
    "    # Select the nearest pixel to the chosen point\n",
    "    rain_series = ds.sel(\n",
    "        x=ds[\"x\"].sel(x=pt[\"lon\"], method=\"nearest\"),\n",
    "        y=ds[\"y\"].sel(y=pt[\"lat\"], method=\"nearest\")\n",
    "    )[\"precip_intensity\"]\n",
    "    \n",
    "    # Store the series in the dictionary with the point's name\n",
    "    data_dict[pt[\"name\"]] = rain_series.values\n",
    "\n",
    "# 5. Create a DataFrame: rows = timestamps, columns = points\n",
    "df_points = pd.DataFrame(data_dict, index=rain_series[\"time\"].values)\n",
    "df_points.index.name = \"time\"  # Name the index for clarity\n",
    "\n",
    "# 6. Display the final DataFrame\n",
    "print(\"Final DataFrame: each column is a point, each row is a timestamp\")\n",
    "df_points\n",
    "\n",
    "# 7. Save the DataFrame as an Excel file in the notebook folder\n",
    "# excel_path = Path(\".\") / \"hsaf_points_timeseries.xlsx\"\n",
    "# df_points.to_excel(excel_path, index=True)\n",
    "# print(f\"DataFrame saved to {excel_path}\")\n",
    "\n",
    "df_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter the DataFrame for this date\n",
    "# df_date = df_points[df_points.index.date == pd.to_datetime(date_to_plot).date()]\n",
    "\n",
    "df_date = df_points\n",
    "# Create the figure\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Create a vector for the X-axis\n",
    "x = np.arange(len(df_date.index))\n",
    "n_locations = len(df_date.columns)  # number of locations\n",
    "width = 0.8 / n_locations  # width of bars distributed among locations\n",
    "\n",
    "# Loop over all columns (locations)\n",
    "for i, col in enumerate(df_date.columns):\n",
    "    # Plot bars for the intensity\n",
    "    ax1.bar(\n",
    "        x + i * width,                # offset bars for each location\n",
    "        df_date[col].values,          # intensity values (mm/h)\n",
    "        width=width,\n",
    "        label=col\n",
    "    )\n",
    "\n",
    "# Format the X-axis\n",
    "ax1.set_xticks(x + width * (n_locations - 1) / 2)\n",
    "ax1.set_xticklabels(df_date.index.strftime(\"%Y-%m-%d %H:%M\"), rotation=45, ha=\"right\")\n",
    "ax1.set_ylabel(\"Precipitation intensity (mm/h)\")\n",
    "ax1.set_xlabel(\"Time\")\n",
    "\n",
    "# Convert to mm per 15-minute step\n",
    "df_mm = df_date * 0.25  # mm/h * 0.25 = mm/15min\n",
    "\n",
    "# Add a second Y-axis for cumulative sum\n",
    "ax2 = ax1.twinx()\n",
    "for col in df_mm.columns:\n",
    "    ax2.plot(\n",
    "        x, \n",
    "        df_mm[col].cumsum(), linestyle=\"--\", label=f\"Cumulative {col} (mm)\"\n",
    "    )\n",
    "ax2.set_ylabel(\"Cumulative sum (mm)\")\n",
    "\n",
    "# Merge legends\n",
    "lines_labels = [ax.get_legend_handles_labels() for ax in [ax1, ax2]]\n",
    "lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
    "ax1.legend(lines, labels, loc=\"upper left\")\n",
    "\n",
    "# Title and layout\n",
    "plt.title(f\"Precipitation with cumulative sum\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ICPAC Course (pixi)",
   "language": "python",
   "name": "icpac-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
